# @package _global_

# to execute this experiment run:
# python run.py experiment=template

defaults:
  - override /datamodule: cxr.yaml
  - override /model: cxr.yaml
  - override /callbacks: wandb.yaml
  - override /logger: wandb
  - override /trainer: default.yaml

# we override default configurations with nulls to prevent them from loading at all
# instead we define all modules and their paths directly in this config,
# so everything is stored in one place

# name of the run determines folder name in logs
# it's also accessed by loggers
name: "chexpert_dense_net"

seed: 1

trainer:
  _target_: pytorch_lightning.Trainer
  gpus: 1
  min_epochs: 1
  max_epochs: 100
  gradient_clip_val: 0.5
  accumulate_grad_batches: 2
  weights_summary: "full"
  num_sanity_val_steps: 0

model:
  _target_: src.models.cxr_module.CXRModel
  model: "MIMIC_NB" #"Densenet121" "Densenet161" "Densenet169" "Densenet201"
                #"RSNA" "NIH" "PadChest" "CheXpert" "MIMIC_NB" "MIMIC_CH" "ALL" 
  
  pretrained: True
  drop_rate: 0
  optimizer: "Adam" #"RMSprop" "SGD"
  lr: 3e-4
  weight_decay: 0.0005
  momentum: 0               #RMSprop    #SGD
  eps: 1e-08        #Adam   #RMSprop
  alpha: 0.99               #RMSprop
  dampening: 0                          #SGD
  betas: [0.9, 0.999] #Adam
  centered: False           #RMSprop
  nesterov: False                       #SGD
  amsgrad: False      #Adam

datamodule:
  _target_: src.datamodules.cxr_datamodule.CXRDataModule
  model: ${model.model}
  data_dir: ${data_dir}
  batch_size: 16
  num_workers: 20
  pin_memory: False
  classes: 2
  dataset_size: 2021     #0 = all
  train_val_test_split: [70, 20, 10]
  img_size: 128   # DOESN'T AFFECT XRV MODELS
  normal: 0       #[0]-MIN/(MAX-MIN)*255, 1-Histogram, 2-CLAHE
  scale: 0
  shear: 0
  translation: 0
  rotation: 0
  horizontal_flip: False
  vertical_flip: False

callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "val/acc"
    save_top_k: 2
    save_last: True
    mode: "max"
    dirpath: "checkpoints/"
    filename: "sample-mnist-{epoch:02d}"
  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: "val/acc"
    patience: 10
    mode: "max"

logger:
  wandb:
    _target_: pytorch_lightning.loggers.wandb.WandbLogger
    project: "multi"
    notes:  
    name: 
    save_dir: "./"
    offline: False # set True to store all logs only locally
    id: # pass correct id to resume experiment!
    log_model: True
    job_type: "train"